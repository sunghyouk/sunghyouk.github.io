---
title:  "Mathematics for machine learning - Chapter 2: Linear algebra"
excerpt: "Linear algebra"

categories:
  - MATH
tags:
  - [mathematics, linear_algebra, MML-book]
use_math: true

date: 2021-10-17
last_modified_at: 2021-10-17

toc: true
toc_sticky: true

---

## Chapter 2: Linear algebra

선형 대수는 `vector`에 대한 학문이고 `vector`를 다루는 규칙에 대한 학문이다.  
일반적으로 `vector`는 같은 종류의 다른 객체를 만들기 위해 더할 수 있고 스칼라를 곱할 수 있다.

여러 벡터의 예

1. Geometric vectors: 우리에게 친숙한 vector
2. Polynomials
3. Audiosiganls
4. Elements of $\mathbb{R}^n$ - 이 책에서 집중하고 있는 개념  

### 2.1 Systems of linear equations

Geometric Interpretation of systems of linear equations의  측면에서 each linear equation defines a line on the $x_1$, $x_2$ -plane.  

We collect the coefficients $a_{ij}$ into vectors and collect the vectors in matrices.

### 2.2 Matrices

used to compactly represent systems of linear equations, but they also represent linear functions  

`Def 2.1` (Matrix)

#### 2.2.1 Matrix addition and multiplication

- element wise sum
- dot product  

```python
c = np.einsum('il, lj', A, B)
```

*cf)* elementwise multiplication - `Hadamard product`  

`Def 2.2` (Identity Matrix)

- $\mathbb{R}^{n\times n}$ as the $n\times n$ -matrix containing 1 on the diagonal and 0 everywhere else.

matrix multiplication은 not commutative, associativity, distributivity 성립  

#### 2.2.2 Inverse and transpose

`Def 2.3` (Inverse)  
$\mathbf{AB=I_n=BA}$ : called the inverse of $\mathbf{A}$ and denoteb by $\mathbf{A^{-1}}$  
행렬 $\mathbf{A}$ 가 역행렬 $\mathbf{A^{-1}}$ 을 갖는다면 *regular/invertible/nonsigular*  

inverse to each other since $\mathbf{AB=I=BA}$  

`Def 2.4` (Transpose)  
`Def 2.5` (Symmetric matrix)  
A metrix $\mathbf{A}\in \mathbb{R}^{n\times n}$ is symmetric if $\mathbf{A=A^{T}}$  

`Remark` 대칭행렬의 합은 언제나 대칭행렬이지만, 곱은 일반적으로 대칭행렬이 나오지 않는다.  

#### 2.2.3 Multiplication by a scalar

#### 2.2.4 Compact representations of systems of linear equation

### 2.3 Solving systems of linear equations

solving system에 집중하고 역행렬의 탐색에 대한 알고리즘을 제공하는 것에 초점을 둔다.

#### 2.3.1 Particular and general solution

1. find a particular solution to $\mathbf{Ax=b}$
2. find all solution to $\mathbf{Ax=0}$
3. combine the solutions from step 1 and 2 to the general solution.

neither the general nor the particular solution is unique.

#### 2.3.2 Elementary transformations

*augmented matrix* $\mathbf{A\vert b}$ : used the vertical line to separate the left-hand side from the right-hand side.  

각 행을 곱하고 더해서 요약하기 쉽다. (*row-echelon form*)  

이것을 다시 변수와 함께 explicit notation with variable 표현 방식으로 바꾸면 쉽게 풀린다.  

`Remark` (Pivots and staircase structure)  
좌변에 첫 0이 아닌 수 (leading coefficient of a row): pivot이라 부르고, 항상 그 위 행의 pivot의 오른쪽에 있다.  
그러므로 row-echelon form의 방정식 구조는 "staircase" 구조를 가진다.  

`Def 2.6` (row-echelon form)

- 모든 행의 성분이 0만 있는 행은 행렬의 바닥에 위치한다.  
- 0 성분이 아닌 행을 보자면, 좌항에 첫 0이 아닌 수는 pivot 또는 leading coefficient라 부르고 항상 그 위 행의 pivot의 오른쪽에 있다.  

`Remark` (basic and free variable)  
row-echelon form에서 pivot에 해당하는 변수는 `basic` variable, 그 외는 `free variable`이라 불린다.  

`Remark` (Obtaining a particular solution)  
pivot column을 이용해 equation systme의 우측을 표현할 수 있다.  
non-pivot columns의 계수를 암시적으로 0으로 설정하는 것이 문제를 풀기 쉽다는 것을 기억하라.  

`Remark` (Reduced row-echelon form)  

- row-echelon form의 일종이다.
- 모든 pivot = 1
- pivot이 그 column에서 유일한 non-zero  

`Remark` (Gaussian elimination)  
linear equation을 reduced row-echelon form으로 만들기 위해 elementary transformation을 수행하는 알고리즘이다.  

reduced row-echelon form으로 변환되면 쉽게 해를 구할 수 있다.

#### 2.3.3 The minus-1 trick

reduced row-echelon form인 $\mathbf{A}$ 의 해를 찾아보자.  
$\mathbf{A}\in \mathbb{R}^{k\times n}, x \in \mathbb{R}^n$ 인 행렬에서,  
$j_1\, ...,\, j_k$ 열에서 pivot인 standard unit vector $e_1,\, ...,\, e_k \in \mathbb{R}^k$ 라고 하면,  
이 행렬에 $n-k$ 행을 더해 $\tilde{A} = n\times n$ 행렬로 만들 수 있다.  

그러므로, 대각 성분이 -1, 1의 값을 갖게 된다.  
예를 들어,  
$$\mathbf{A}=\left[
  \begin{matrix}
  1 & 3 & 0 & 0 & 3 \\
  0 & 0 & 1 & 0 & 9 \\
  0 & 0 & 0 & 1 & -4 \\
  \end{matrix}
  \right] $$

pivot colum이 아닌 2, 5열 -> 2행, 5행에 대각 성분이 -1이 되도록 $\left[\begin{matrix}0 & -1 & 0 & 0 & 0\end{matrix}\right]$ 을 삽입 대각에 -1이 들어간 열이 해이다.  즉,  

$$\left\{x \in \mathbb{R}^5:\, x = \lambda_1 \left[
  \begin{matrix}
  3 \\ -1 \\ 0 \\ 0 \\ 0
  \end{matrix}
  \right]
  + \lambda_2 \left[
    \begin{matrix}
    3 \\ 0 \\ 9 \\ -4 \\ -1
    \end{matrix}
    \right], \, \lambda_1, \lambda_2 \in \mathbb{R}
  \right\}$$

이는 역행렬을 구할 때도 사용할 수 있다.  
$[\mathbf{A}\vert \mathbf{I}_n]$ - augmented matrix 형태로 변환 - Gaussian 소거법으로 reduced REF로 변환 - $[\mathbf{I}_n\vert \mathbf{A}^{-1}]$

#### 2.3.4 Algorithms for solving a system of linear equations

$$\mathbf{A}x = b \Leftrightarrow \mathbf{A}^T\mathbf{A}x = \mathbf{A}^Tb \Leftrightarrow x = (\mathbf{A}^TA)^{-1}\mathbf{A}^Tb$$

이는 minimum norm least-square solution에 해당하며 *Moore-Penrose pseudo-inverse*라고 불린다.  

### 2.4 Vector spaces

#### 2.4.1 Groups

`Def 2.7` (group)  

아래와 같은 특성을 지닐 때 group이라 부른다.  

- 연산에 대해 갇혀 있다.
- 결합 법칙이 성립
- neutral element
- inverse element

`Remark` 역은 연산에 의해 정의되는 것이지 반드시 $\frac{1}{x}$ 이 아니다.  
교환 법칙이 성립되는 경우 *Abelian group*  

`Def 2.8` (general linear group)  
regular (invertible) matrices의 집합 $\mathbf{A}\in \mathbb{R}^{n\times n}$ 은 gorup이다. 행렬 곱셈의 정의가 성립함으로서, 그래서 general linear group *GL (n, $\mathbb{R}$)*.  

#### 2.4.2 Vector spaces

`Def 2.9` (vector space)  
A real-valued *vector space* $V\,=\,(\mathcal{V}, +, \cdot)$ is a set $\mathcal{V}$ with two operations  

`Remark` vector space는 정의하였지만, multiplication (outer product)은 아직 정의 되지 않음  
`example` + (inner operation), scalar 곱 (outer operation)은 항상 성립하고, 복소수에서도 성립함  

#### 2.4.3 Vector subspaces

`Def 2.10` (vectpr subspace)  
$\mathcal{U}\, \subseteq\, \mathcal{V},\,\mathcal{U}\neq\emptyset$ then $U=(\mathcal{U},\,+,\,\cdot)$ is called *vector subspace*  
`Remark` 모든 subspace는 $U\,\subseteq\,(\mathbb{R}^n,\,+,\,\cdot)$ homogeneous linear equation의 homogeneous system의 해 공간이다.  

### 2.5 Linear independence

`Def 2.11` (linear combination)  
`Def 2.12` (linear (in)dependence)  

쉽게 생각해서,  
$\mathbf{0}=\Sigma_{i=1}^k\lambda_i\mathbf{x}_i$ 를 만족하는 $\lambda$ 에서 적어도 하나의 $\lambda_i \neq 0$ 이면 *independent*, 모든 $\lambda=0$ 이면 *dependent*  

linear combination으로 벡터를 만들어 내게 되면 구성에 사용된 벡터들은 linearly dependent하게 된다.  

`Remark` 벡터가 linearly independent한가를 알아보려면:

- 벡터들은 독립 혹은 종속 두 가지만 있다. 세번째 선택지는 없다.
- 벡터 중 적어도 하나가 $\mathbf{0}$ 이면, linearly dependent
- 만약, 그리고 벡터 중 적어도 하나가 다른 벡터의 linear combination일 때 linearly dependent
- Gaussian elimination을 사용해 보자: 모두가 pivot column이면 선형 독립이다.

`Remark` 벡터 공간 $V$ 에서, $m > k$ 이면, $k$ 벡터의 선형 조합 $m$ 이 선형 독립이다.  

### 2.6 Basis and Rank

#### 2.6.1 Generating set and basis

`Def 2.13` (generating set and span)  
벡터 $\mathcal{A}$ 의 모든 벡터가 그 구성 벡터로 표현될 수 있을 때 $V$ 의 *generating set*이라고 하며, 그 벡터의 모든 선형 조합의 집합을 *span*이라 부른다.  
즉, generating set은 벡터 공간을 span하는 벡터의 집합이다.  
`Def 2.14` (basis)  
$V$ 를 span 하는 벡터 중 이보다 작은 벡터가 없으면 $\mathcal{A}$ 는 *minimal*이라 한다. 모든 선형 독립의 generating set은 minimal이고 *basis*라 부른다.  

`Remark` 모든 벡터 공간 $V$ 는 기저 $\mathcal{B}$ 를 지니고 있다. 벡터 공간 $V$ 의 많은 기저를 가질 수 있다. 그러나 모든 기저는 같은 원소수를 가진다. 이를 *basis vector*라 한다.  

차원은 basis vector의 수이다. 이를 dim($V$)으로 표기한다.  
`Remark` basis vector의 원소 수와 차원은 다를 수 있다.  

basis를 결정하는 방법:

1. 행렬 $\mathbf{A}$ 의 열로서 spanning vector를 쓴다.
2. 행사다리꼴 행렬을 결정한다.
3. pivot 열과 연관된 spanning vector가 basis이다.

#### 2.6.2 Rank

선형 독립인 열과 선형 독립인 행이 같으면 이를 *rank*로 부르고 `rk(A)`로 표시한다.  

`Remark` 중요한 특성 몇 가지:

- column rank와 row rank는 같다.
- $A\, \in\, \mathbb{R}^{m\times n}$ 의 열이 아공간을 span할 때 dim($U$) = rk($A$)라 할 수 있다.
  - 이를 *image* 또는 *range*로 부름
  - $U$의 기저는 pivot 열을 알아 내기 위해 $A$에 Gaussian 소거법을 적용해 발견할 수 있다.

- 행도 위와 똑같이 적용 가능
- *kernel* 또는 *null space*
- *full rank*: 만약 rank가 같은 차원의 행렬에 대해 가장 클 가능성이 있다면 full rank를 가졌다고 한다.

### 2.7 Linear mappings

`Def 2.15` (linear mappings - 선형 사상):  
linear mapping (or linear transformation)  

$$ \forall x, y\in V\,\forall \lambda, \psi\in\mathbb{R}\, :\, \Phi(\lambda x +\psi y)\, =\, \lambda\Phi(x) + \psi\Phi(y). $$  

`Def 2.16` (injective, surjective, bijective):  

- Injective (단사) $\forall x, y \in \mathcal{V}: \Phi(x)=\Phi(y) \Rightarrow x = y$
- Surjective (전사) $\Phi(\mathcal{V})=\mathcal{W}
- Bijective if it is injective and surjective

- isomorphism: linear and bijective  

`Theorem 2.17` dim($V$) = dim($W$)라면 그리고 이럴 때에만 유한한 차원의 벡터 공간 V와 W는 isomorphic하다.  
직관적으로 생각해 보면, 같은 차원의 벡터 공간이 어떤 손실을 초래하지 않고 서로 변환이 가능하다는 이야기이다.  

#### 2.7.1 Matrix representation of linear mappings

- 선형 사상의 행렬 표현  

$B = (\mathbf{b_1, ..., b_n})$ n 차원의 벡터 공간 V의 기저를 순서대로 나열한 것: *ordered basis* (순서 기저)라고 함  
`Def 2.18` (coordinates - 좌표): 선형 조합으로 순서 기저를 이용해 유일한 표현식을 얻었을 때 이를 좌표 벡터 (coordinate vector)라 한다.  
기저는 좌표계를 효율적으로 정의하지만, 꼭 표준 기저를 선택해 벡터를 표현할 필요는 없다.  

`Def 2.19` (transformation matrix): 선형 사상으로 옮길 때 옮겨 가는 좌표 평면 상의 순서 기저로 유일한 표현이 가능하다면 이를 transformation matrix로 부름  

#### 2.7.2 Basis change

`Remark` $x$ 벡터는 변경하지 않고, ($e_1,\, e_2$)로 각각 좌표로 사상된 것을 ($b_1,\, b_2$)로 각각 좌표로 사상된 것으로 바꾼다. 그렇게 기저가 바뀌면서 벡터의 행렬 표현이 바뀌게 될텐데, 컴퓨터 계산이 더 용이해지도록 transformation matrix가 변할 수 있다.  

`Def 2.21` (equivalence): $\tilde{A}=T^{-1}AS$  
`Def 2.22` (similarity): $\tilde{A}=S^{-1}AS$  
`Remark` similar 행렬은 항상 equivalent하지만, equivalent 행렬이 similar하지는 않다.  

#### 2.7.3 Image and Kernel

선형 사상의 상 (image)과 핵 (kernel)은 어떤 중요한 속성을 지닌 벡터 아공간이다.  
`Def 2.23` (image and kernel):  
핵과 영공간은 다음과 같이 정의한다.  
$$ker(\Phi):=\Phi^{-1}(\mathbf{0}_W)=\{\upsilon\in V\, :\, \Phi(\upsilon)=\mathbf{0}_W\}$$  
상과 range는 다음과 같이 정의한다.  
$$Im(\Phi):=\Phi(V)=\{\omega\in W|\exists\upsilon\in V:\, \Phi(\upsilon)=\omega\}$$  

`Remark`: $\mathbf{0}_v \in ker(\Phi)$  
$Im(\Phi)\subseteq W$ , $ker(\Phi)\subseteq V$  
$ker(\Phi)=\{\mathbf{0}\}$ 라면 그리고 그럴 때에만 $\Phi$는 injective하다.  

`Remark`: null space와 column space의 특징  
`Theorem 2.24` (Rank-nullity theorem): 벡터 공간 V, W 그리고 선형 사상 $\Phi:V\rightarrow W$에 대해서 다음이 성립한다.  
$$dim(ker(\Phi))+dim(Im(\Phi))=dim(V)$$  

### 2.8 Affine spaces

`Remark` distinction between linear and affine is sometimes not clear

#### 2.8.1 Affine subspaces

`Def 2.25` (Affine subspace)

#### 2.8.2 Affine mappings

`Def 2.26` (Affine mapping)