---
layout  : wiki
title   : "Threats to federated learning"
summary : 
date    : 2022-01-31 10:49:09 +0900
updated : 2022-02-03 18:00:11 +0900
tag     : #Federated_learning, #Attacks, #Privacy, #Robustness
toc     : true
public  : true
parent  : [[2022-01-31-FL_2]]
latex   : false
---

# Threats to federated learning

## Abstract

Existing FL protocol design has been shown to exhibit vulnerabilities which can be exploited by adversaries both within and outside of the system to compromise data privacy.  
Two major attacks on FL: 1) poisoning attacks and 2) inference attacks  

## 1. Introductionc

Federated learning which pushes model training to the devices from which data originate emerged as a promising alternative ML paradigm.  
It can handle unbalanced and non-independent and identically distributed (non-IID) data which naturally arise in the real world.  

### 1.1 Types of FL

Under HFL, datasets owned by each participant share similar features but concern different users.  
* H2B (Business): small numbers of participants, training frequently, high technical capability - *Example)* Google's GBoard application
* H2C (Consumer): large numbers of participants, not frequent training, low technical capability

VFL is applicable to the cases in which participants have large overlaps in the sample space but differ in the feature space. VFL mainly targets business participants.  
FTL deals with scenarios in which FL participants have little overlap in both the sample space and the feature space.

### 1.2 Threats to FL

A more recent work showed that the malicious attacker can completely steal the training data from gradients in a few iteration (NOTE: Next article).  
FL protocol desings may contain vulnerabilities for both (1) the (potentially malicious) server, (2) any participant.  
In particular, we focus on two specific threats initiated by the insiders on FL systems: 1) **poisoning attacks**; and 2) **inference attacks**.  

## 2. Threat models

We first present a summary of the threat models.

### 2.1 Inisder v.s. Outsider

Insider attacks are generally stronger than the outsider attacks.  
1) Single attack: a single, non-colluding malicious participant aims to cause the model to miss-classify a set of chosen inputs with high confidence.
2) Byzantine attack: the byzantine malicious participants may behave completely arbitrarily and tailor their outputs to have similar distribution as the correct model updates, making them difficult to detect.
3) Sybil attack: the adversaries can simulate multiple dummy participant accounts or select previously compromised participants to mount more powerful attacks on FL.

### 2.2 Semi-honest v.s. Malicious

Under the semi-honest setting, adversaries are considered passive or hnest-but-curious.  
Under the malicious setting, an active, or malicious adversary tries to learn the private states of honest participants, and deviates arbitrarily from the FL protocol by modifying, re-playing, or removing messages.

### 2.3 Training phase v.s. Inference phase

Attacks at training phase attempt to learn, influence, or corrupt the FL model itself.  
Attacks at inference phase are called evasion/exploratory attacks. - either cause it to produce wrong output or collect evidence about the model characteristics.  
Classified into white-box attacks and black-box attacks.

## 3. Poisoning attacks

Random attacks aim to reduce the accuracy of the FL model,  
Targeted attacks aim to induce the FL model to output the target label specified by tha adversary.  

1) data poisoning attack during local data collection;
2) model poisoning attack during local model training process.

### 3.1 Data poisoning

Clean-label attacks assume that the adversary cannot change the label of any training data as there is a process by which data are certified as belonging to the correct class and the poisoning of data samples has to be imperceptible.  
In dirty-label poisoning, the adversary can introduce a number of data sample it wishes to miss-classify with the desired target label into the training set.  

* label-flipping attack
* backdoor poisoning

Data poisoning is less effective in settings with fewer participants like H2C.

### 3.2 Model poisoning

Model poisoning attacks aim to poison local model updates before sending them to the server or insert hidden backdoors into the global model

## 4. Inference attacks

Model updates can leak extra information about the unintended features about participants' training data to the adversarial participants.  
The adverasary can also save the snapshot of the FL model parameters, and conduct property inference by exploiting the difference between the consecutive snapshots.  

### 4.1 Inferring class representatives

*Example)* Generative adversarial networks (GAN) attack assumes that the entire training corpus for a given class comes from a single participant, and only in the special case where all class members are similar, GAN-constructed representatives are similar to the training data.  
This resembles model inversion attacks.

### 4.2 Inferring membership

Given an exact data point, membership inference attacks aim to determine if it was used to train the model.  
In the active case, the attacker can tamper with the FL model training protocol and performs a more powerful attack against other participants.  
This attack called gradient ascent attack.

### 4.3 Inferring properties

to infer properties of other participants' training data that are independent of the features that characterize the classes of the FL model.  
assumes that the adversary has auxillary training data correctly labelled with the property he wants to infer.  

### 4.4 Inferring training inputs and labels

Deep Leakage from Gradient (DLG) and improved DLG  
iDLG is valid for any differentiable model trained with cross-entropy loss over one-hot labels.  
