---
layout  : wiki
title   : "Federated learning Chapter 2."
summary : "Chapter 2. Background"
date    : 2022-01-13 13:26:41 +0900
updated : 2022-01-19 15:48:06 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 2. Background

## 2.1 Privacy-preserving machine learning

Data leakage and privacy violation incidents have brought about heightened public awareness of the need for AI systems to be able to preserve user privacy and data confidentiality.  
그 결과로, privacy-preserving machine learning systems (PPML)로 알려진 체계가 나왔다.  
일반적으로 사용자 개인정보와 데이터 보안을 보호하기 위한 방어 방법을 탑재한 ML을 포괄적으로 일컫는 용어이다.  

이 장에서는 개인 정보 보호 모델 훈련과 추론에 대한 주요 개념인 **secure multi-party computation (MPC)**, **homomorphic encryption (HE)**, 원치않는 데이터 노출을 방지하기 위한 **differential privacy (DP)** 의 개념을 소개한다.  

## 2.2 PPML and secure ML

Secure ML - 공격자는 데이터 분석 체계의 integrity, availability를 공격한다.  
PPML - 공격자는 ML 체계의 privacy, confidentiality를 공격한다.  

세 가지 주요 공격 유형:  
1) integrity attack: 있는데 없다고 분류된다. 즉, 정상으로 분류하게 만듦
2) availability attack: false positive, false negative 모두 가능, 모델을 못 쓰게 만듦
3) confidentiality attack: 민감 정보를 유출

이 장에서는 주로 PPML과 방어 기술에 초점을 둔다.

## 2.3 Threat and security models

### 2.3.1 Privay threat models

참여자의 주된 세 가지 역할:  
1) input party - 예를 들어 데이터 소유자
2) computation party - 예를 들어 모델 설계자, 추론 서비스 제공자
3) result party - 예를 들어 모델 querier, 사용자

공격자는 어디든 공격할 수 있다.  

* Reconstruction attacks - ML 모델 훈련이나 추론 동안 훈련 데이터나 특징 벡터를 추출하고자 하는 시도를 말한다. 
  * FL에서 model update와 gradient information만 집적 서버와 교류하더라도 명백한 특성값인 SVM이나 kNN 값은 교류하지 않도록 해야 방어할 수 있다.
  * 이를 방지하기 위한 방법으로 MPC, HE으로 중간 값을 private하게 유지할 수 있다. - black-box access를 보장
  > GOTO: Section 2.4.1, Section 2.4.2
* Model inversion attacks - 공격자는 모델에 white-box, black-box 접근이 모두 가능하다고 가정한다.
  * white-box 접근: 저장된 특징 벡터 없이 clear-text 모델을 알고 있다.
  * black-box 접근: 데이터로 모델에 질문을 하고 반응을 얻는 방식으로 공격, 물론 *equation solving attack* 으로 clear-text를 얻을 수 있다.
  * 이를 방지하기 위해, 모델의 정보를 공격자에게 적게 노출되도록 해야한다. - 신뢰 구간을 반올림해서 제공, HE와 혼합된 bayesian neural network 방법을 사용할 수 있다.
* Membership-inference attacks - 공격자의 목표는 모델의 훈련 셋 내 샘플이 있는가를 아는 것이다.
  * 공격을 방해하기 위해 반올림된 예측 결과를 보고함으로서 결과 일반화와 같은 전략이 효율적이다.
  * DP는 이 공격을 막기 위한 주된 방법이다. 
  > GOTO: Section 2.4.3
* Attribute-inference attacks - 공격자는 탈익명화 혹은 기록 소유자를 목표로 한다.
  * PII (personally identifiable information)을 공개 전에 삭제하는 것은 자연스러워 보이지만 효과가 별로 없다.
  * 방어하기 위해 group anonymization privacy 접근법을 사용할 수 있다.
* Model poisoning attacks - backdoor attack이라는 공격에 연합 학습이 취약할 수 있다.
  * 방어하기 위해 blockchain-based 방법, trusted execution environment (TEE) based 방법이 있다.

### 2.3.2 Adversary and security models

공격자의 두가지 유형:  
* Semi-honest adversaries - 받은 정보로 부터 받은 출력을 넘어서 더 많은 정보를 학습하고자 한다.
  * 대부분의 PPML 연구에서 이를 대부분 고려한다.
    * ML 프로토콜을 정직하게 따르는 파티가 이득을 얻는다. - 악성 공격자는 아무 이득을 얻지 못한다.
    * 암호화에서도 이를 상정하고 protocol secure를 구축하는 것이 표준적인 방법이다.
* Malicious adversaries - 프로토콜에서 빗겨나서 임의로 행동한다. 
* 두 모델 모두에서 공격자는 파티의 부분을 오염시키고, 오염된 파티는 서로 공모할 수 있다.

## 2.4 Privacy preservation techniques

### 2.4.1 Secure multi-party computation

MPC, secure function evaluation (SFE)로도 알려져 있다.  
목적: 다른 파티에 입력을 노출하지 않고, 각 파티에 의해 개인정보보호된 입력을 넣어 합동으로 함수를 계산하는 것이다.  

#### Definition

각 파티는 함수의 출력값만 알 수 있다.  
*simulation paradigm* 을 응용하여 MPC protocol을 증명할 수 있다.  
세 가지 다른 framework가 있다: 1) Oblivious Transfer (OT), 2) Secret Sharing (SS), 3) Threshold Homomorphic Encryption (THE)이 있으나, SS가 널리 MPC의 핵심으로 간주된다.  

#### Oblivious Transfer

두 파티 간 computation protocol이다.  
보내는 사람은 message-index pair를 보내고, 각 전송에 받는 사람은 그 중 하나의 인덱스를 선택한다.  
수신자는 데이터베이스에 대해 어떤 정보도 얻을 수 없고, 보내는 사람은 수신자의 선택에 대해 알 수 없다.  

**Definition 2.1** 1-out-of-n OT: Party A가 입력으로 $(x_1,\, ...,\,x_n)$을 갖고 있다고 하고, Party B는 $i\in 1,\, ...,\,n$을 입력으로 갖고 있다고 할 때, A는 $i$에 대해 알 수 없고 B는 $x_i$에 대해 알 수 없다.  

Construction of OT: Bellare-Micali's, Naor-Pinka's, and Hazay-Lindall's approach 등이 있다.  
Bellare-Micali's construction은 다음과 같이 작동한다:  
1) 수신자는 두개의 공개 키를 발신자에게 보낸다.
2) 그 중 하나의 개인정보보호 키를 갖고만 있는다. 그러나 발신자는 어떤 공개 키를 보냈는지 모른다.
3) 그리고 발신자는 해당하는 공개키를 암호화하고 수신자에게 ciphertext를 보낸다.
4) 수신자는 목표 ciphertext를 복호화한다.

* Bellare-Micali's Construction
* Yao's Garbled Circuit (GC) - OT protocol과 block cipher로 이루어져 있다.
* OT extension

#### Secret sharing

무작위 부분과 이 부분을 다른 파티에 분산하는 부분으로 쪼개 비밀 값을 숨기는 개념이다.  
arithmetic secret sharing이 현존하는 SMPC 기반 PPML 방법에 의해 가장 많이 채택된다.  
*semi-honest dealer* 없이 offline phase를 수행하고자 한다면 몇 가지 프로토콜이 있다. 가령, SPDZ, SPDZ-2, MASCOT 등, 이 중 BGV기반 SHE 프로토콜이 성능이 더 좋았다.

#### Application in PPML

two-phase architecture - offline, onlin phase로 이루어진다.  
암호화 공정의 대부분은 offline phase에서 이루어지며 multiplication triples가 생성된다. 이를 이용해 online phase에서 모델 훈련이 된다.  
**DeepSecure** - 보안 신경망 추론을 위한 GC 기반 프레임워크  
통신 비용은 circuit 내 AND의 전체 숫자에만 의존한다.  
**SecureML**  
**Chameleon framework**  
OT에 기반한 개인정보보호 ID3 학습은 Shamir's threshold secret sharing을 사용, 개인정보 입력 모델의 평균을 평가하고 모델 업데이트에 대해 파라미터에 평균을 공개한다.  

### 2.4.2 Homomorphic encryption

PPML에서 MPC의 대체제로 인식한다. 앞선 섹션에서 이야기한 MPC를 수행하기 위해 사용될 수 있다.  
2009년 완성되었다.

#### Definition

$\mathcal{H}$은 해당 ciphertext에 효율적인 조작을 적용하여, 암호화된 내용이 수행되도록 어떤 산술적 조작을 허용하게 하는 암호화 스키마이다.

$$\mathcal{H}=\left\{KeyGen, Enc, Dec, Eval\right\}$$

- *KeyGen*: Key generation.
  * asymmetric HE: $\left\{pk, sk\right\}=KeyGen(g)$, $pk$는 일반 텍스트의 암호화에 대해 공개키이고 $sk$는 암호화 텍스트의 복호화를 위한 비밀키이다.
  * symmetric HE: 비밀키만 생성된다.
- *Enc*: Encryption.
  * asymmetric: 공개키 - *pk*, 일반 텍스트 *m* 를 입력으로 받아서 암호화 텍스트 $c=Enc_{pk}(m)$을 출력으로 한다.
  * symmetric: 비밀키 - *sk*, 일반 텍스트 *m* 을 입력으로 받아서 암호화 텍스트 $c=Enc_{sk}(m)$을 출력으로 한다.
- *Dec*: Decryption. - 대칭이건 비대칭이건 *sk* 와 암호화 텍스트 *c* 를 입력으로 받아 $m=Dec_{sk}(c)$ 생산하도록 한다.
- *Eval*: Evaluation. - 암호화 텍스트 *c* 와 공개키 *pk* 를 입력으로 받아 함수화된 일반 텍스트에 해당하는 암호화 텍스트를 출력으로 한다.  
  
다음 조건을 만족하면 *homomorphic* 이라 불리는 보안 암호 체계이다:  
$$\forall m_1,\,m_2\in \mathcal{M},\, Enc_{enk}(m_1\odot_\mathcal{M}m_2)\leftarrow Enc_{enk}(m_1)\odot_\mathcal{C}Enc_{enk}(m_2)$$

#### Categorization of HE schemes

세 가지 클래스로 분류된다.  
1) Partially HE
2) Somewhat HE
3) Fully HE
기능성이 증가할 수록 계산상의 복잡도도 증가한다.  

* Partial HE: $(\mathcal{M},\,\odot_\mathcal{M})$ and $(\mathcal{C},\,\odot_\mathcal{C})$ 모두 군이다. $\odot_\mathcal{C}$를 암호화 텍스트에 대해 무제한으로 적용가능하다. group homomorphism technique
* Somewhat HE: 덧셈과 곱셈에 대해 제한된 횟수에만 적용가능, 보안을 위해 잡음을 증가시킨다. 곱셈이 잡음을 증가시키기 위한 주된 기법이다.
* Fully HE: 암호화 텍스트에 대해 무제한의 덧셈, 곱셈 연산자를 허용한다.  
    * 네 가지의 주요 가족이 있다.  
        * Ideal Lattice-based FHE
        * Approximate-GCD based FHE
        * (R)LWE-based FHE
        * NTRU-like FHE
    * circular security을 가정하고 고비용의 bootstrap 연산을 고안하여 SHE에 기반을 둔 현존하는 FHE는 실제 사용에서 너무 느리고 일반적인 MPC 에 대해 경쟁력이 없다.

#### Application in PPML

FedMF uses Paillier's HE for secure federated matrix factorization assuming honest-but-curious server and honest clients.  
Secure federated transfer learning is studied via Paillier's HE scheme, where the semi-honest thrid party is into the discard by mixing HF with additive secret sharing in decryption process.

### 2.4.3 Differential privacy (DP)

DP was originally developed to faciliate secure analysis over sensitive data.  
The key idea of DP is to confuse the adversaries when they are trying to query individual information from the database so that adversaries cannot distinguish individual-level sensitivity from the query result.

#### Definition

DP can be used to resist the membership inference attack.  
The $(\epsilon-\delta)$-differential privacy is defined as follows:  
**Definition 2.2** $(\epsilon-\delta)$-differential privacy. A randomized mechanism $\mathcal{M}$ preseve DP if given any two datasets *D* and *D'* differing by only one record, and for all $S \subset Range(\mathcal{M})$,  
$$Pr\left[\mathcal{M}(d)\in S\right]\leq Pr\left[\mathcal{M}(D^\prime) \in S \right] \times e^\epsilon + \delta$$
where $\epsilon$is the privacy budget, $\delta$ is the failure probability.  
The quantity $\ln\frac{Pr\left[\mathcal{M}(D)\in S\right]}{Pr\left]\mathcal{M}(D^\prime)\in S\right]}$ is called the *privacy loss*  
privacy를 크게 할 수록 accuracy loss는 커진다.

#### Categorization of DP schemes

There are mainly two ways to achieve DP by adding noise to the data. - adding noise according to the sensitivity of a function, or choosing noise according th an exponential distribution among discrete values.  
**Definition 2.3** Sensitivity. 오로지 한 기록에 의해 두 데이터셋 *D* 와 *D'* 이 다르고 함수 $\mathcal{M}:\, \mathcal{D}\rightarrow\mathcal{R}^d$ 임의의 정의역에 대해, $\mathcal{M}$의 민감도는 모든 가능한 입력에 대해 $\mathcal{M}$의 출력에 최대한의 변화이다.  
$$\Delta\mathcal{M}=\max_{D,D^\prime}\lVert\mathcal{M}(D)-\mathcal{M}(D^\prime)\rVert,$$
$\lVert\cdot\rVert$는 벡터의 노름이다.  
**Theorem 2.4** 임의의 정의역 D에 대해 주어진 함수 $\mathcal{M}:\, \mathcal{D}\rightarrow\mathcal{R}^d$에 모든 입력 X에 대해 함수는 다음과 같이 정의된다.  
$$\mathcal{M}(X)+Lap\left( \frac{\Delta\mathcal{M}}{\epsilon} \right) ^d$$
$\epsilon$-DP는 독립적으로 생성된 Laplace noise를 더하여 달성될 수 있다. - Gaussian이나 binomial noise도 될 수 있지만, 약하다.  

exponential mechanism을 사용하여 DP를 달성하는 방법:  
주어진 데이터베이스와 $\epsilon$ 파라메터에 대해, quality function (*q*)는 출력 정의역에 대해 확률분포를 유도한다.  
**Definition 2.5** $q:\,(\mathcal{D}^n \times \mathcal{R})\rightarrow\mathbb{R}$을 quality function이라 하고, 주어진 데이터셋 $d\in\mathcal{D}^n$, 각 결과에 점수를 할당, $r \in\mathcal{R}$.
$S(q)=\max_{r,D,D^\prime}\lVert q(D,r)-q(D^\prime,r)\rVert_1$이라하자.  
$$\mathcal{M}(d,q)=\left\{ return\, r\, with\, probability \propto \exp\left( \frac{\epsilon q(d,r)}{2S(q)}\right)\right\}$$

어떻게 어디에서 섭동이 적용되는가에 따라 분류할 수 있다:  
1) Input perturbation
2) Objective perturbation
3) Algorithm perturbation
4) Output perturbation

#### Application in PPML

local DP가 사용될 수 있다. 입력측에서는 데이터를 교란시켜서 신뢰가 없는 서버에 혼란된 데이터를 전달한다. - randomized response (RR)  
GAN을 사용할 수도 있다.  
Moments accountant: DP SGD를 위해 제안되었다, 고려 중인 특정 잡음을 설명함으로서 훈련 중인 신경망 모델에서 전체 privacy cost를 계산한다.  
LSTM language model: 예측 정확도에서 무시할만한 비용에서만 사용자 수준의 DP 보장을 하도록 구축되었다.  
pCDBN (private convolutional deep belief network)  
client-level DP는 충분히 많은 참여자가 연합학습에 참여할 때 사용하기 쉽고 정확도가 높다.  

