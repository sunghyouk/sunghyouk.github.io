---
layout  : wiki
title   : "Federated learning Chapter 2."
summary : "Chapter 2. Background"
date    : 2022-01-13 13:26:41 +0900
updated : 2022-01-13 15:43:25 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 2. Background

## 2.1 Privacy-preserving machine learning

Data leakage and privacy violation incidents have brought about heightened public awareness of the need for AI systems to be able to preserve user privacy and data confidentiality.  
그 결과로, privacy-preserving machine learning systems (PPML)로 알려진 체계가 나왔다.  
일반적으로 사용자 개인정보와 데이터 보안을 보호하기 위한 방어 방법을 탑재한 ML을 포괄적으로 일컫는 용어이다.  

이 장에서는 개인 정보 보호 모델 훈련과 추론에 대한 주요 개념인 **secure multi-party computation (MPC)**, **homomorphic encryption (HE)**, 원치않는 데이터 노출을 방지하기 위한 **differential privacy (DP)** 의 개념을 소개한다.  

## 2.2 PPML and secure ML

Secure ML - 공격자는 데이터 분석 체계의 integrity, availability를 공격한다.  
PPML - 공격자는 ML 체계의 privacy, confidentiality를 공격한다.  

세 가지 주요 공격 유형:  
1) integrity attack: 있는데 없다고 분류된다. 즉, 정상으로 분류하게 만듦
2) availability attack: false positive, false negative 모두 가능, 모델을 못 쓰게 만듦
3) confidentiality attack: 민감 정보를 유출

이 장에서는 주로 PPML과 방어 기술에 초점을 둔다.

## 2.3 Threat and security models

### 2.3.1 Privay threat models

참여자의 주된 세 가지 역할:  
1) input party - 예를 들어 데이터 소유자
2) computation party - 예를 들어 모델 설계자, 추론 서비스 제공자
3) result party - 예를 들어 모델 querier, 사용자

공격자는 어디든 공격할 수 있다.  
* Attribute-inference attacks - 


