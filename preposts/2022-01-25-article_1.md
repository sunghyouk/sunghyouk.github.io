---
layout  : wiki
title   : 
summary : 
date    : 2022-01-25 15:05:52 +0900
updated : 2022-01-25 15:58:15 +0900
tag     : 
toc     : true
public  : true
parent  : [[index]]
latex   : false
---

# Privacy-preserving deep learning via weight transmission

## Abstract

This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method, because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from the existing systems in the following features: 1) any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; 2) gradients computed by SGD are not shared but the weight parameters are shared instead; and 3) robustness against colluding parties even in the extreme case that only one honest party exists. One of our systems requires a shared symmetric key among the data owners (trainers) to ensure the secrecy of the weight parameters against a central server. We prove that our systems, while privacy preserving, achieve the same learning accuracy as SGD and, hence, retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets and show that our systems outperform the previous system in terms of learning accuracies.

* scenario SGD algorithm
* differ from any activation function can be used, no privacy-preserving-friendly approximation is required
* weight parameters are shared
* robustness against colluding parties
* achieve the same learning accuracy

## IV. Our propsed systems: privacy-preserving SGD via weight transmission

one with a Server-aided Network Topology (SNT) and the other with  Fully-connected Network Topology (FNT) in which each connection is via a separate TLS/SSL channel.  
The SNT system makes use of an honest-but-curious server while the FNT system does not.  
The datasets are horizontally distributed.  

### A. Our SNT system

There is one common server and multiple distributed trainers. The server is assumed to be honest-but-curious. Each trainer is connected with the server via a separate communication channel such as a TLS/SSL channel.  
The symmetric key $K$ is shared between trainers and is kept secret to the server. Notationally, $Enc_K(\cdot)$ is a symmetric encryption with key $K$, which protects the weight vector against the honest-but-curious server.  
The training data of the trainers is used only locally, while the encrypted weight vector $Enc_K(W)$ is sent back and forth between the server and the trainers. The weight vector $W$ needs to be initialized once, which can be carried out by one of the trainers by simply initializing $W$ randomly and sending $Enc_K(W)$ to the server.

### B. Our Fully-connected network topology (FNT) system

There is no central server; thus there is no need to use symmetric encryption to encrypt the weights, but the trainers still need secure channels such as TLS to transfer the weights to each other. Because there is no central server, the trainers must agree the order of sending and receiving weight parameters in advance or simply do transmission in a random order.  
Each trainer receives a weight from the previous trainer, and updates that weight using mini-batches of data from its own datset, then sends the updated weight to the next trainer.  

### C. Additional considerations for our SNT and FNT systems

## V. Experiments

To show the general applicapability of systems, we conduct experiments on various datasets including UCI datsets, and image datasets (MNIST, CIFAR-10, and CIFAR-100).

