---
layout  : wiki
title   : "An introduction to GLM Chapter 11"
summary : "Chapter 11. Clustered and longitudinal data"
date    : 2022-02-14 06:22:36 +0900
updated : 2022-02-14 12:18:44 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-introduction_GLM]]
latex   : false
---

## 11.1 Introduction

The term repeated measures is used to describe both longitudinal and clustered data.  
1) Dropping the usual assumption of independence - repeated measures and the generalized estimating equation approach
2) multilevel modelling - outcomes at the same level are assumed to be independent and the correlation is a result of the multilevel structure.

It is necessary to choose a correlation structure likely to reflect the relationships between the observations.  
Correlation parameters - in order to obtain consistent estimates of those parameters that are of interest and to correctly calculate the SE of these estimates.  

Multilevel models - by fixed parameters (e.g., for group effect) or random variables (E.g., for subjects randomly allocated to groups). - mixed model  

## 11.2 Example: Recovery from stroke

A naive analysis, sometimes called a pooled analysis, of these data is to fit an analysis of covariance model.  
* Equation 11.1  
$$E(Y_{ijk}) = \alpha_i + \beta t_k + e_{ijk}$$

The slopes may differ between the groups  
* Equation 11.2  
$$E(Y_{ijk}) = \alpha_i + \beta_i t_k + e_{ijk}$$

## 11.3 Repeated measures models for Normal data

A normal linear model for $\mathbf{y}$ is  
* Equation 11.3
$$E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta} = \boldsymbol{\mu};\, \mathbf{y}\sim MVN(\boldsymbol{\mu}, \mathbf{V})$$

The maximum likelihood estimator is obtained by solving the score equations  
* Equation 11.4
$$\mathbf{U}(\boldsymbol{\beta})=\frac{\partial{l}}{\partial{\boldsymbol{\beta}}}=\mathbf{X}^T\mathbf{V}^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})=\sum_{i=1}^N \mathbf{X}_i^T\mathbf{V}_i^{-1}(\mathbf{y}_i - \mathbf{X}_i\boldsymbol{\beta})=\mathbf{0}$$

The solution is  
* Equation 11.5
$$\hat\boldsymbol\beta=(\mathbf{X}^T\mathbf{V}^{-1}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{V}^{-1}\mathbf{y}=(\sum_{i=1}^N \mathbf{X}_i^T\mathbf{V}_i\mathbf{X}_i)^{-1}(\sum_{i=1}^N\mathbf{X}_i^T\mathbf{V}_i^{-1}\mathbf{y}_i)$$

with  
$$var(\hat\boldsymbol\beta)=(\mathbf{X}^T\mathbf{V}^{-1}\mathbf{X})^{-1}=(\sum_{i=1}^N\mathbf{X}_i^T\mathbf{V}_i^{-1}\mathbf{X}_i)^{-1}$$

and $\hat\boldsymbol\beta$ is asymptotically Normal (NOTE: Chapter 6)  

$\mathbf{V}_s(\boldsymbol\beta)$ is called the **information sandwich estimator**, because $\mathfrak{J}$ is the information matrix (NOTE: Chapter 5).  
**Huber estimator** is a consistent estimator of var$(\hat\boldsymbol\beta)$ when $\mathbf{V}$ is not known, and it is robust to mis-specification of $\mathbf{V}$.  

1. All the off-diagonal elements are equal  
   * Equation 11.7
   * clusted data where it is plausible that all measurements are equally correlated
   * $\rho$: intra-class correlation coefficient
2. off-diagonal terms decrease with distance between observation  
   * first order autoregressive model with $\rho^{|j-k|}$, where $|\rho|<1$
   * Equation 11.8, 11.9
3. All the correlation terms may be different  
   * Unstructured correlation matrix

Some programs treat repeated measures as a special case of multivariate data - This is especially inappropriate for longitudinal data in which the time order of the observations matters. (REF: https://onlinelibrary.wiley.com/doi/epdf/10.1002/%28SICI%291097-0258%2820000330%2919%3A6%3C861%3A%3AAID-SIM407%3E3.0.CO%3B2-F)
