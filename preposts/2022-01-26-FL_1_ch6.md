---
layout  : wiki
title   : "FL Chapter 6"
summary : "Chapter 6. Federated transfer learning"
date    : 2022-01-26 12:33:54 +0900
updated : 2022-01-27 14:30:17 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 6. Federated transfer learning

transfer knowledge among the parties to achieve better performance.

## 6.1 Heterogeneous FL

* Datasets may share only a handful of samples and features.
* Distributions among those datasets could be quite different.
* The size of those datasets could vary greatly.
* Some participants may only have data with no or limited labels.

## 6.2 Federated transfer learning

Transfer learning is a learning technique to provide solution for cross-domain knowledge transfer.  
The essence of TL is to find the invariant between a resource-rich source domain and a resource-scarce target domain.

* Instance-based FTL:  
    * HFL: Participating parties can selectively pick or re-weight training data samples to relieve the distribution difference.
    * VFL: Participating parties can selectively choose features and samples for avoiding negative transfer.  
* Feature-based FTL: 
    * HFL: The common feature representation space can be learned through minimizing the maximum mean discrepancy (MMD).
    * VFL: The common feature representation space can be learned through minimizing the distance between representation of aligned samples
* Model-based FTL:
    * HFL: a kind of model-based FTL - is served as a pre-trained model to be finetuned
    * VFL: for inferring missing features and labels

Formally, FTL aims to provide solutions for situation when (Equation 6.1):  
$$\mathcal{X}_i\neq\mathcal{X}_j,\, \mathcal{Y}_i\neq\mathcal{Y}_j,\, \mathcal{I}_i\neq\mathcal{I}_j,\, \forall\mathcal{D}_i,\mathcal{D}_j,i\neq j$$
The objective is to predict labels for newly incoming samples or existing unlabeled samples as accurately as possible.  

*Definition 6.1* **Security definition of a FTL system** - source domain party and the target domain party. All parties in the federation follow the federation protocols and rules but they will try to deduce information from data received.
For a protocol $P$ performing $(O_A,O_B)=P(I_A,I_B)$, where $O_A$ and $O_B$ are party A's and party B's respective outputs, and $I_A$ and $I_B$ are their respective inputs, $P$ is secure against party A if there exists an infinite number of $(I_B',O_B')$ pairs such that $(O_A,O_B')=P(I_A,I_B')$.

## 6.3 The FTL framework

Without loss of generality, we assume all labels are in party A. And, A and B already found or both know their commonly shared sample IDs.  
* Figure 6.2 illustrates the architecture of two neural networks.

Optimization problem for model training using the available labeled dataset (Equation 6.2):
$$\min_{\Theta^A,\Theta^B}\mathcal{L}_1=\sum_i^{N_c}\ell_1(y_i^A,\varphi(u_i^B))$$
where $\Theta^A$, $\Theta^B$ are training parameters of $Net^A$ and $Net^B$.  
In addition, we also aim to minimize the alignment loss between A and B (Equation 6.3):
$$\min_{\Theta^A,\Theta^B}\mathcal{L}_2=-\sum_i^{N_{AB}}\ell_2(u_i^A,u_i^B)$$
where $\ell_2 denotes the alignment loss.  
The final objective function for model training is (Equation 6.4):
$$\mathcal{L}=\mathcal{L}_1+\gamma\mathcal{L}_2+\frac{\lambda}{2}(\mathcal{L}_3^A+\mathcal{L}_3^B)$$
where $\gamma$ and $\lambda$ are the weight parameters.  
The next step is to obtain the gradients (Equation 6.5):
$$\frac{\partial\mathcal{L}}{\partial\theta_l^i}=\frac{\partial\mathcal{L_1}}{\partial\theta_l^i}+\frac{\partial\mathcal{L}_2}{\partial\theta_l^i}+\lambda\theta_l^i$$
