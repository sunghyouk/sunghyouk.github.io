---
layout  : wiki
title   : "FL Chapter 6"
summary : "Chapter 6. Federated transfer learning"
date    : 2022-01-26 12:33:54 +0900
updated : 2022-01-28 14:23:50 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 6. Federated transfer learning

transfer knowledge among the parties to achieve better performance.

## 6.1 Heterogeneous FL

* Datasets may share only a handful of samples and features.
* Distributions among those datasets could be quite different.
* The size of those datasets could vary greatly.
* Some participants may only have data with no or limited labels.

## 6.2 Federated transfer learning

Transfer learning is a learning technique to provide solution for cross-domain knowledge transfer.  
The essence of TL is to find the invariant between a resource-rich source domain and a resource-scarce target domain.

* Instance-based FTL:  
    * HFL: Participating parties can selectively pick or re-weight training data samples to relieve the distribution difference.
    * VFL: Participating parties can selectively choose features and samples for avoiding negative transfer.  
* Feature-based FTL: 
    * HFL: The common feature representation space can be learned through minimizing the maximum mean discrepancy (MMD).
    * VFL: The common feature representation space can be learned through minimizing the distance between representation of aligned samples
* Model-based FTL:
    * HFL: a kind of model-based FTL - is served as a pre-trained model to be finetuned
    * VFL: for inferring missing features and labels

Formally, FTL aims to provide solutions for situation when (Equation 6.1):  
$$\mathcal{X}_i\neq\mathcal{X}_j,\, \mathcal{Y}_i\neq\mathcal{Y}_j,\, \mathcal{I}_i\neq\mathcal{I}_j,\, \forall\mathcal{D}_i,\mathcal{D}_j,i\neq j$$
The objective is to predict labels for newly incoming samples or existing unlabeled samples as accurately as possible.  

*Definition 6.1* **Security definition of a FTL system** - source domain party and the target domain party. All parties in the federation follow the federation protocols and rules but they will try to deduce information from data received.
For a protocol $P$ performing $(O_A,O_B)=P(I_A,I_B)$, where $O_A$ and $O_B$ are party A's and party B's respective outputs, and $I_A$ and $I_B$ are their respective inputs, $P$ is secure against party A if there exists an infinite number of $(I_B',O_B')$ pairs such that $(O_A,O_B')=P(I_A,I_B')$.

## 6.3 The FTL framework

Without loss of generality, we assume all labels are in party A. And, A and B already found or both know their commonly shared sample IDs.  
* Figure 6.2 illustrates the architecture of two neural networks.

Optimization problem for model training using the available labeled dataset (Equation 6.2):
$$\min_{\Theta^A,\Theta^B}\mathcal{L}_1=\sum_i^{N_c}\ell_1(y_i^A,\varphi(u_i^B))$$
where $\Theta^A$, $\Theta^B$ are training parameters of $Net^A$ and $Net^B$.  
In addition, we also aim to minimize the alignment loss between A and B (Equation 6.3):
$$\min_{\Theta^A,\Theta^B}\mathcal{L}_2=-\sum_i^{N_{AB}}\ell_2(u_i^A,u_i^B)$$
where $\ell_2 denotes the alignment loss.  
The final objective function for model training is (Equation 6.4):
$$\mathcal{L}=\mathcal{L}_1+\gamma\mathcal{L}_2+\frac{\lambda}{2}(\mathcal{L}_3^A+\mathcal{L}_3^B)$$
where $\gamma$ and $\lambda$ are the weight parameters.  
The next step is to obtain the gradients (Equation 6.5):
$$\frac{\partial\mathcal{L}}{\partial\theta_l^i}=\frac{\partial\mathcal{L_1}}{\partial\theta_l^i}+\frac{\partial\mathcal{L}_2}{\partial\theta_l^i}+\lambda\theta_l^i$$

### 6.3.1 Additively homomorphic encryption

Applying Equation (6.4) and (6.5), and additively HE (denoted as $\left[[\cdot\right]]$), we obtain the privacy preserved loss function and the corresponding gradients for the two domain as: from Equation 6.6 through Equation 6.8

### 6.3.2 The FTL training process

* Step 1: Party A and B initialize and run their independent neural networks $Net^A$ and $Net^B$ locally to obtain hidden representations $u_i^A$ and $u_i^B$.
* Step 2: Party A computes and encrypts a list of intermediate components, denoted as $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^B})^A\right]\right]_A$ sends them to B to assist with the calculations of gradients $\frac{\partial\mathcal{L}}{\partial\theta_l^B}$. While Party B computes and encrypts a list of intermediate components, denoted as $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^A})^B\right]\right]_B$, $\left[[\mathcal{L}^B\right]]_B$, and sends them to A to assist with the calculations of gradients $\frac{\partial\mathcal{L}}{\partial\theta_l^A}$ and loss $\mathcal{L}$.
* Step 3: Based on $\frac{\partial\mathcal{L}}{\partial\theta_l^A}$ and loss $\left[[\mathcal{L}^B\right]]_B$ received, Party A computes $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^A})\right]\right]_B$ and $\left[[\mathcal{L}\right]]_B$ via (6.6) and (6.8). Then Party A creates random mask $m^A$ and add it to $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^A})\right]\right]_B$ to obtain $\left[\left[\frac{\partial\mathcal{L}}{\partial\theta_l^A}+m^A\right]\right]_B$. Party A sends $\left[\left[\frac{\partial\mathcal{L}}{\partial\theta_l^A}+m^A\right]\right]_B$ and $\left[[\mathcal{L}\right]]_B$ to B. Based on $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^B})^A\right]\right]_A$ received, Party B computes $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^B})\right]\right]_A$ via Equation 6.7. Then Party B creates random mask $m^B$ and add it to $\left[\left[(\frac{\partial\mathcal{L}}{\partial\theta_l^B})\right]\right]_A$ to obtain $\left[\left[\frac{\partial\mathcal{L}}{\partial\theta_l^B}+m^B\right]\right]_A$ and sends to Party A.
* Step 4: Party A decrypts $\frac{\partial\mathcal{L}}{\partial\theta_l^B}+m^B$ and sends it to Party B. While Party B decrypts, $\frac{\partial\mathcal{L}}{\partial\theta_l^A}+m^A$ and $\mathcal{L}$, and sends them to Party A.
* Step 5: Party A and Party B remove random masks and obtain gradients $\frac{\partial\mathcal{L}}{\partial\theta_l^A}$ and $\frac{\partial\mathcal{L}}{\partial\theta_l^B}$. Then the two parties update their respective model with the decrypted gradients.
* Step 6: Party A sends termination signals to Party B once the loss $\mathcal{L}$ converges.

Recently, there are a large number of works discussing the potential risks associated with indirect privacy leakage through gradients.  
To prevent the two parties from knowing each other's gradients, further mask their own gradient with an encrypted random value.

### 6.3.3 The FTL prediction process

Provide predictions for unlabeled data in Party B.  
* Step 1: Party B computes $u_j^B$ with the trained nueral network parameters $\Theta^B$, and sends encrypted $\left[[u_j^B\right]]$ to Party A.
* Step 2: Party A evaluates $u_j^B$ and mask the result with a random value, and sends the encrypted and masked $[[\varphi(u_j^B)+m^A]]_B$ to Party B.
* Step 3: Party B decrypts $[[\varphi(u_j^B)+m^A]]_B$ and sends $\varphi(u_j^B)+m^A$ back to Party A.
* Step 4: Party A obtains $\varphi(u_j^B)$ and the label $y_j^B$, and sends the label $y_j^B$ to Party B.

여기서 performance loss는 second-order Taylor approximation이지 nonlinear activation layer에서 일어나지 않는다. 그러므로 scalable, flexible하다.  

### 6.3.4 Security analysis

*reference* Definition 6.1
At the end of the training process, each party (A or B) remains oblivious to the data structure of the other party and each obtains model parameters associated only with its own features.  
At inference time, the two parties need to collaborate in order to compute the prediction results.

### 6.3.5 Secret sharing-based FTL

HE techniques typically need extensive computational resources and massive parallelization to scale, which make them impractical in many applications that require real-time throughput.
Secret sharing: i) there is no accuracy loss, and ii) computation is much more efficient than HE approach.  
The secret sharing based-FTL training process:  
* Step 1: Party A and Party B initialize and run their independent neural networks locally to obtain hidden representations.
* Step 2: Party A and Party B collaboratively computes $\mathcal{L}_{AB}$ through secret sharing. Party A computes $\mathcal{L}_A$ and sends it to Party B. Party B computes $\mathcal{L}_B$ and sends it to Party A.
* Step 3: Party A and Party B individually reconstruct loss $\mathcal{L}$ via Equation (6.9).
* Step 4: Party A and Party B collaboratively computes $\left(\frac{\partial\mathcal{L}}{\partial\theta_\ell^A}\right)_{AB}$ and $\left(\frac{\partial\mathcal{L}}{\partial\theta_\ell^B}\right)_{AB}$ through secret sharing.
* Step 5: Party A computes its gradient via $\frac{\partial\mathcal{L}}{\partial\theta_\ell^A}=\left(\frac{\partial\mathcal{L}}{\partial\theta_l^A}\right)_A+\left(\frac{\partial\mathcal{L}}{\partial\theta_\ell^A}\right)_{AB}$ and updates its local model $\theta_\ell^A$. While at the same time, Party B computes its gradients via $\frac{\partial\mathcal{L}}{\partial\theta_\ell^B}=\left(\frac{\partial\mathcal{L}}{\partial\theta_l^B}\right)_B+\left(\frac{\partial\mathcal{L}}{\partial\theta_\ell^B}\right)_{AB}$ and updates it local model $\theta_\ell^B$;
* Step 6: Party A sends termination signals to Party B once the loss $\mathcal{L}$ converges.

Prediction phase:  
* Step 1: Party A and Party B run their trained neural networks locally to obtain hidden representations.
* Step 2: Party A and Party B collaboratively reconstruct $\varphi(u_j^B)$ through secret sharing and calculate the label $y_j^B$.

## 6.4 Challenges and outlook

* We need to develop schemes to learn the transferable knowledge in a way that it can well capture the invariant between participants.
* We need to determine how to learn a representation of transfer knowledge in a distributed environment while preserving the privacy of the shared representation of all participants.
* We need to design efficient secure protocols that can be employed in FTL.

