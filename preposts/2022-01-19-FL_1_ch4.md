---
layout  : wiki
title   : "Federated learning Chapter 4"
summary : "Chapter 4. Horizontal FL"
date    : 2022-01-19 16:38:48 +0900
updated : 2022-01-20 17:24:43 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 4. Horizontal FL

## 4.1 The definition of HFL

HFL, sample-partitioned FL, or example-partitioned FL: datasets at different sites share overlapping feature space but differ in sample space (Figure 4.1)  
We summarize the conditions for HFL as:  
* Equation (4.1)
$$\mathcal{X}_i=\mathcal{X}_j,\, \mathcal{Y}_i=\mathcal{Y}_j,\, I_i\neq I_j,\, \forall\mathcal{D}_i,\mathcal{D}_j,i\neq j$$
where the data feature space and label space pair of the two parties are assumed to be the same, whereas the user identifiers $I_i$ and $I_j$ are assumed to be different; $\mathcal{D}_i$ and $\mathcal{D}_j$ denote the datasets of the $i$th party and the $j$th party, respectively.  

**Security of an HFL system.** only the server can compromise the user privacy and data security of the participants.
최근, secure client-server structure, 연합학습이 사용자에 의해 데이터를 쪼개서 전역 연합 모델을 구축하기 위해 서버 측에 협력하여 클라이언트 기기에서 모델을 구축하는 것을 허용하는 구조가 제안되었다.  

## 4.2 Architecture of HFL

### 4.2.1 The client-server architecture

* Figure 4.2

Master-worker architecture. $K$ participants (also known as clients or users or parties) with the same data structure collaboratively train a ML model with the help of a server (also known as parameter server or aggregation server or coordinator).  
A typical assumption is that the participants are honest whereas the server is honest-but-curious.  
Therefore, the aim is to prevent leakage of information from any participants to the server.  

* Step 1: Participants locally compute training gradients, mask a selection of gradients with encryption, differential privacy, or secret sharing techniques, and send the masked results to the server.
* Step 2: The server performs secure aggregation
* Step 3: The server sends back the aggregated results to the participants.
* Step 4: The participants update their respective models with the decrypted gradients.

Iteration until the loss function converges or until the maximum number of allowable iteration is reached or until the maximum allowable training time is reached.  
*gradient averaging* - synchronous stochastic gradient descent (SGD) or federated SGD (FedSGD)  
*model averaging* - gradient descent 대신 model weight를 업데이트  
both gradient averaging and model averaging are referred to as **FedAvg**  

* Table 4.1

similar to the architecture of a DML system, especially the *data-parallel* paradigm  
> GOTO: Section 3.2

For HFL, a data owner is working party:  
It has full *autonomy* to operate on its local data, and can decide when and how to join and contribute to an system.  

The data held of the different participants are not identically distributed in most of the practical applications.  

### 4.2.2 The peer-to-peer architecture

There is no central server or coordinator. The $k$ participants are called trainers, or distributed trainers or workers.  
The trainers need secure channels to transfer the model weights to each other.  
* Figure 4.3

The trainers must agree to the order of sending and receiving model weights in advance.  
* Cyclic transfer: The trainers are organized into a chain. The top of the chain sends its current model weights to its downstream trainer. It updates the received model weights using mini-batches of training data from its own datasets.
* Random transfer: The $k$th trainer selects a number $i$ from $\left\{1,...,L\right\} \backslash \{K\}$ at random with equal probablility, also known as Gossip Learning

Advantage: the possibility to remove the central server  
Disadvantage: weight parameters are updated serially rather than in parallel batches, which takes more time to train a model.  

### 4.2.3 Global model evaluation

Each participant can easily test the model performance using its **local testing** dataset to get the local model performances, but it takes more efforts to get the global model performance across all participants.  

* Local model performance means the performance of an HFL model examined on the *local test dataset* of *single participant*,
* Global model performance refers to the performance of an HFL model evaluated on the test datasets of *all the participants*.

During model training process and after model training being completed in HFL, we can obatain the global model performance:  

For the *client-server architecture*:  
* Step 1. The $k$th participant evaluates the performance of the current HFL model using its local test dataset.
* Step 2. The $k$th participant sends the local model evaluation results to the coordinator.
* Step 3. The coordinator can calculate the global model performance. For example, the global model recall can be computed as: $\frac{\sum_{k=1}^KN_{TP}^k}{\sum_{k=1}^K(N_{TP}^k+N_{FN}^k)}$ (중앙 서버에 테스트 데이터가 있어서 테스팅을 하는 것이 아님)
* Step 4. The coordinator sends the computed global model performance back to all the participants.

For the *peer-to-peer architecture*:  
Since there is no central coordinator, it would be more complicated to obtain global model performance.  
One possible way is to pick one of the trainers to serve as a *temporary* coordinator. - recommended for evaluating the final HFL model  
One possible way to remedy this issue is to ask the trainers to take turns to act as the temporary coordinator.

## 4.3 The Federated Averaging algorithm

In McMahan et al., the federated averaging (**FedAvg**) algorithm was employed for federated model training in HFL systems. - parallel restarted SGD and local SGD.  
> Federated learning: Strategies for improving communication efficiency, 2016. http://arxiv.org/abs/1610.05492  
> Federated optimization: Distributed machine learning for on-device intelligence, 2016. http://arxiv.org/abs/1610.02527

### 4.3.1 Federated optimization

The optimization problem arising from FL has several key properties:  
* Non-independent identical distribution (Non-IID) of datasets. The data owned by different participants may follow completely different distributions.
* Unbalanced number of data points.
* Huge number of participants.
* Slow and unreliable communication links.

The focus of FedAvg is on non-convex objective functions. FedAvg is applicable to any finite-sum objective function:  
* Equation 4.2
$$f(w)=\frac{1}{n}\sum_{i=1}^n f_i(w)$$
where $n$ - number of data points and $w\in R^d$ - model parameters of dimension $d$.  
$f_i(w)=\mathcal{L}(x_i,y_i;w)$: loss of the prediction on sample $(x_i, y_i)$ for the given parameters.  
* Equation 4.3
$$f(w)=\sum_{k=1}^K \frac{n_k}{n}F_k(w)\,where\,F_k(w)=\frac{1}{n_k}\sum_{i\in\mathcal{P}_k} f_i(w)$$

One round: refers to the operation of sending updates from the participants to the server and from the server back to the participants.  
In FL, communication costs dominate as communication takes place over the Internet or WANs, even with wireless and mobile networks. (computation cost is negligible)  
To add computation:  
* Increased parallelism
* Increased computation on each participant

### 4.3.2 The FedAvg algorithm


