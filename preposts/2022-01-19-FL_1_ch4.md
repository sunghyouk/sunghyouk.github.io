---
layout  : wiki
title   : "Federated learning Chapter 4"
summary : "Chapter 4. Horizontal FL"
date    : 2022-01-19 16:38:48 +0900
updated : 2022-01-19 18:05:55 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 4. Horizontal FL

## 4.1 The definition of HFL

HFL, sample-partitioned FL, or example-partitioned FL: datasets at different sites share overlapping feature space but differ in sample space (Figure 4.1)  
We summarize the conditions for HFL as:  
* Equation (4.1)
$$\mathcal{X}_i=\mathcal{X}_j,\, \mathcal{Y}_i=\mathcal{Y}_j,\, I_i\neq I_j,\, \forall\mathcal{D}_i,\mathcal{D}_j,i\neq j$$
where the data feature space and label space pair of the two parties are assumed to be the same, whereas the user identifiers $I_i$ and $I_j$ are assumed to be different; $\mathcal{D}_i$ and $\mathcal{D}_j$ denote the datasets of the $i$th party and the $j$th party, respectively.  

**Security of an HFL system.** only the server can compromise the user privacy and data security of the participants.
최근, secure client-server structure, 연합학습이 사용자에 의해 데이터를 쪼개서 전역 연합 모델을 구축하기 위해 서버 측에 협력하여 클라이언트 기기에서 모델을 구축하는 것을 허용하는 구조가 제안되었다.  

## 4.2 Architecture of HFL

### 4.2.1 The client-server architecture

* Figure 4.2

Master-worker architecture. $K$ participants (also known as clients or users or parties) with the same data structure collaboratively train a ML model with the help of a server (also known as parameter server or aggregation server or coordinator).  
A typical assumption is that the participants are honest whereas the server is honest-but-curious.  
Therefore, the aim is to prevent leakage of information from any participants to the server.  

* Step 1: 
