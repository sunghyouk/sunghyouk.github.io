---
layout  : wiki
title   : "Federated learning Chapter 4"
summary : "Chapter 4. Horizontal FL"
date    : 2022-01-19 16:38:48 +0900
updated : 2022-01-24 11:39:38 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-FL_1]]
latex   : false
---

# Chapter 4. Horizontal FL

## 4.1 The definition of HFL

HFL, sample-partitioned FL, or example-partitioned FL: datasets at different sites share overlapping feature space but differ in sample space (Figure 4.1)  
We summarize the conditions for HFL as:  
* Equation (4.1)
$$\mathcal{X}_i=\mathcal{X}_j,\, \mathcal{Y}_i=\mathcal{Y}_j,\, I_i\neq I_j,\, \forall\mathcal{D}_i,\mathcal{D}_j,i\neq j$$
where the data feature space and label space pair of the two parties are assumed to be the same, whereas the user identifiers $I_i$ and $I_j$ are assumed to be different; $\mathcal{D}_i$ and $\mathcal{D}_j$ denote the datasets of the $i$th party and the $j$th party, respectively.  

**Security of an HFL system.** only the server can compromise the user privacy and data security of the participants.
최근, secure client-server structure, 연합학습이 사용자에 의해 데이터를 쪼개서 전역 연합 모델을 구축하기 위해 서버 측에 협력하여 클라이언트 기기에서 모델을 구축하는 것을 허용하는 구조가 제안되었다.  

## 4.2 Architecture of HFL

### 4.2.1 The client-server architecture

* Figure 4.2

Master-worker architecture. $K$ participants (also known as clients or users or parties) with the same data structure collaboratively train a ML model with the help of a server (also known as parameter server or aggregation server or coordinator).  
A typical assumption is that the participants are honest whereas the server is honest-but-curious.  
Therefore, the aim is to prevent leakage of information from any participants to the server.  

* Step 1: Participants locally compute training gradients, mask a selection of gradients with encryption, differential privacy, or secret sharing techniques, and send the masked results to the server.
* Step 2: The server performs secure aggregation
* Step 3: The server sends back the aggregated results to the participants.
* Step 4: The participants update their respective models with the decrypted gradients.

Iteration until the loss function converges or until the maximum number of allowable iteration is reached or until the maximum allowable training time is reached.  
*gradient averaging* - synchronous stochastic gradient descent (SGD) or federated SGD (FedSGD)  
*model averaging* - gradient descent 대신 model weight를 업데이트  
both gradient averaging and model averaging are referred to as **FedAvg**  

* Table 4.1

similar to the architecture of a DML system, especially the *data-parallel* paradigm  
> GOTO: Section 3.2

For HFL, a data owner is working party:  
It has full *autonomy* to operate on its local data, and can decide when and how to join and contribute to an system.  

The data held of the different participants are not identically distributed in most of the practical applications.  

### 4.2.2 The peer-to-peer architecture

There is no central server or coordinator. The $k$ participants are called trainers, or distributed trainers or workers.  
The trainers need secure channels to transfer the model weights to each other.  
* Figure 4.3

The trainers must agree to the order of sending and receiving model weights in advance.  
* Cyclic transfer: The trainers are organized into a chain. The top of the chain sends its current model weights to its downstream trainer. It updates the received model weights using mini-batches of training data from its own datasets.
* Random transfer: The $k$th trainer selects a number $i$ from $\left\{1,...,L\right\} \backslash \{K\}$ at random with equal probablility, also known as Gossip Learning

Advantage: the possibility to remove the central server  
Disadvantage: weight parameters are updated serially rather than in parallel batches, which takes more time to train a model.  

### 4.2.3 Global model evaluation

Each participant can easily test the model performance using its **local testing** dataset to get the local model performances, but it takes more efforts to get the global model performance across all participants.  

* Local model performance means the performance of an HFL model examined on the *local test dataset* of *single participant*,
* Global model performance refers to the performance of an HFL model evaluated on the test datasets of *all the participants*.

During model training process and after model training being completed in HFL, we can obatain the global model performance:  

For the *client-server architecture*:  
* Step 1. The $k$th participant evaluates the performance of the current HFL model using its local test dataset.
* Step 2. The $k$th participant sends the local model evaluation results to the coordinator.
* Step 3. The coordinator can calculate the global model performance. For example, the global model recall can be computed as: $\frac{\sum_{k=1}^KN_{TP}^k}{\sum_{k=1}^K(N_{TP}^k+N_{FN}^k)}$ (중앙 서버에 테스트 데이터가 있어서 테스팅을 하는 것이 아님)
* Step 4. The coordinator sends the computed global model performance back to all the participants.

For the *peer-to-peer architecture*:  
Since there is no central coordinator, it would be more complicated to obtain global model performance.  
One possible way is to pick one of the trainers to serve as a *temporary* coordinator. - recommended for evaluating the final HFL model  
One possible way to remedy this issue is to ask the trainers to take turns to act as the temporary coordinator.

## 4.3 The Federated Averaging algorithm

In McMahan et al., the federated averaging (**FedAvg**) algorithm was employed for federated model training in HFL systems. - parallel restarted SGD and local SGD.  
> Communication-efficient learning of deep networks from decentralized data https://arxiv.org/abs/1602.05629
> Federated learning of deep networks using model averaging https://pdfs.semanticscholar.org/8b41/9080cd37bdc30872b76f405ef6a93eae3304.pdf

### 4.3.1 Federated optimization

The optimization problem arising from FL has several key properties:  
* Non-independent identical distribution (Non-IID) of datasets. The data owned by different participants may follow completely different distributions.
* Unbalanced number of data points.
* Huge number of participants.
* Slow and unreliable communication links.

The focus of FedAvg is on non-convex objective functions. FedAvg is applicable to any finite-sum objective function:  
* Equation 4.2
$$f(w)=\frac{1}{n}\sum_{i=1}^n f_i(w)$$
where $n$ - number of data points and $w\in R^d$ - model parameters of dimension $d$.  
$f_i(w)=\mathcal{L}(x_i,y_i;w)$: loss of the prediction on sample $(x_i, y_i)$ for the given parameters.  
* Equation 4.3
$$f(w)=\sum_{k=1}^K \frac{n_k}{n}F_k(w)\,where\,F_k(w)=\frac{1}{n_k}\sum_{i\in\mathcal{P}_k} f_i(w)$$

One round: refers to the operation of sending updates from the participants to the server and from the server back to the participants.  
In FL, communication costs dominate as communication takes place over the Internet or WANs, even with wireless and mobile networks. (computation cost is negligible)  
To add computation:  
* Increased parallelism between client-server communication rounds.
* Increased computation on each participant between communication rounds.

### 4.3.2 The FedAvg algorithm

The amount of computation is controlled by three key parametes:  
1) $\rho$, the fraction of clients that perform computation during each round;
2) $S$, the number of training steps each client performs over its local datset during each round;
3) $M$, the mini-batch size used for the client updates.

We may set $M=\infty$ and $S=1$, $\rho$ controls the *global* batch size, with $\rho=1$ corresponding to the full-batch gradient descent using all data held by all participants.  
Coordinator aggregates gradient and applies the update of model weights:  
* Equation 4.4
$$w_{t+1}\leftarrow w_t-\eta\sum_{k=1}^K\frac{n_k}{n}g_k$$
where $\sum_{k=1}^K\frac{n_k}{n}g_k=\nabla f(w_t)$, provided that the data points held at different participants are IID.  
Alternatively, the coordinator can send the averaged gradient $\bar{g_t}=\sum_{k=1}^K\frac{n_k}{n}g_k$ back to the participants - gradient averaging

#### Model averaging
* Equation 4.5
$$\forall{k},\,w_{t+1}^k\leftarrow\bar{w_t}-\eta g_k$$
Each client locally takes one step (or multiple steps) of gradient descent on the current model weights $\bar{w}_t$ using its local data according to Equation (4.5), and sends the locally updated model weights $w_{t+1}^k$ to the server.
* Equation 4.6
$$\bar{w}_{t+1}\leftarrow\sum_{k=1}^K\frac{n_k}{n}w_{t+1}^k$$
The server then takes a weighted average of the resulting models according to Equation (4.6), and sends the aggregated model weights $\bar{w}_{t+1}$ back to the participants.
It has been found out empirically that model averaging based approach works surprisingly well.

### 4.3.3 The secured FedAvg algorithm

Additively homomorphic encryption (AHE) to enhance the security feature of the FedAvg algorithm.  
AHE is a semi-homomorphic encryption algorithm that only supports the addition and multiplication operations.  

Key properties of AHE:  
Let $\left[[u\right]]$ and $\left[[v\right]]$ denote the homomophic encryption of $u$ and $v$, respectively.
* Addition: $Dec_{sk}(\left[[u\right]]\oplus\left[[v\right]])=Dec_{sk}(\left[[u+v\right]])$, where $\oplus$ may represent multiplication of the ciphertexts.
* Scalar multiplication: $Dec_{sk}(\left[[u\right]]\odot n)=Dec_{sk}(\left[[u\cdot n\right]])$, where $\odot$ may represent taking the power of $n$ of the ciphertexts.

## 4.4 Improvement of the FedAvg algorithm

### 4.4.1 Communication efficiency

> Federated learning: Strategies for improving communication efficiency, 2016. http://arxiv.org/abs/1610.05492  
최근 모델에서 DNN의 parameter가 많다 - huge communication cost가 든다.  
Two strategies for computing model weights:
* Sketched updates - participants compute a normal model weight update and perform a compression afterward locally.
* Structured updates - the model weight update is restricted to be of a form that allows for an efficient compression.

DNN model compression - three-stage pipeline:  
1) prune the DNN connections by removing the redundancy, keeping only the most meaningful connections.
2) the weights quantitized so that multiple connections share the same weights, only effective weights are kept.
3) Huffman coding is applied to take advantage of the biased distribution of effective weights.

to bring down communication overhead - deep gradient compression (DGC) approach:  
1) momentum correction
2) local gradient clipping
3) momentum factor masking
4) warm-up training

Quantitization  
To avoid uploading irrelevant model updates

### 4.4.2 Client selection

Two steps:  
1) resource check step. The coordinator sends queries to a random number of participants to ask about their local resources and the size of data that are relevant to the training task.
2) The coordinator uses this information to estimate the time required for each participant to compute model weight update locally, and the time to upload the update.

## 4.5 Related works

Communication is one of the major challenges of FL. An adaptive communication strategy, termed as AdaComm, was proposed to tackle the problem of random communication delays encountered in FL.  
AdaComm is a communication-efficient SGD algorithm for FL, which is particularly suitable for mobile applications.  
The server immediately updates the global model whenever it receives a local model from a client. The communication between the server and client is non-blocking.  
The hyperparameters of the proposed asynchronous algorithm could be difficult to tune in practice.  
Peer-to-peer architecture due to potential privacy leakage. The collaboration graph and the ML models are jointly learned.  
The fully decentralized solution alternates between i) training nonlinear ML models given the graph in a greedy boosting manner, and ii) updating the collaboration graph (with controlled sparsity) when given the ML models.

## 4.6 Challenges and outlook

The first major challenge is the inability to inspect training data, which leads to one of the central problems, namely choosing the hyperparameters of an ML or DL model and setting the optimizers, particularly for training DNN models. - Simulation based approach가 연구 중이다.  
The second challenge is how to effectively motivate companies and organizations to participate in HFL. - We need to devise effective data protection policies, appropriate incentive mechanisms, and buisiness models for HFL.  
The third challenge is how to prevent cheating behaviors from participants.  

We need to develop new ways to avoid over-fitting and to trigger early-stopping.  
We need smart solutions to replace the dropped participants with new participants without affecting the training process and model accuracy.  
We need to develop efficient mechanisms to defend against model poisoning attacks.  

