---
layout  : wiki
title   : "Privacy in the age of medical big data"
summary : 
date    : 2022-01-26 15:00:36 +0900
updated : 2022-01-27 15:58:40 +0900
tag     : 
toc     : true
public  : true
parent  : [[index]]
latex   : false
---

# Privacy in the age of medical big data

## Abstract

Big data has become the ubiquitous watch word of medical innovation. The rapid development of machine-learning techniques and artificial intelligence in particular has promised to revolutionize medical practice from the allocation of resources to the diagnosis of complex diseases. But with big data comes big risks and challenges, among them significant questions about patient privacy. Here, we outline the *legal and ethical challenges* big data brings to patient privacy. We discuss, among other topics, how best to conceive of health privacy; the importance of equity, consent, and patient governance in data collection; discrimination in data uses; and how to handle data breaches. We close by sketching possible ways forward for the regulatory system.

## Main

We examine the host of ethical concerns and legal responses raised.  
We ground these concerns in a discussion of relevant US law, a key feature of the health data world faced by innovators in this space, and make some regulatory recommendations.

## Why do we need big data in health?

All of these uses require very large sets of healthcare data, including how patients have been treated, how patients have responded to treatment, and personal patient information, such as genetic data, family history, health behavior, and vital sign. Without these data, algorithms cannot be trained or evaluated on how they perform following training.  
Finally, these new insights are routinely incorporated back into the clinical care pathway, whether explicitly (in practice guidelines or publications) or implicitly (in the context of recommendations or procedures automatically embedded into EHR systems).

## How to think about health privacy

There are contextual rules about how information can flow that depend on the actors involved, the process by which information is accessed, the frequency of the access, and the purpose of that access. When these contextual rules are contravened, there has been a privacy violation.
Big data settings, however, have a tendency to increase the number of persons affected, the severity of the effects, and the difficulty for aggrieved individuals to engage in preventive or self-help measures.

### Consequentialist concerns

Consequentialist concerns result from negative consequences that affect the person whose privacy has been violated.

### Deontological concerns

It is hard to say that you have been harmed in a consequentialist sense, but many think the loss of control over your data, the invasion, is itself ethically problematic even when harm is absent.

## Gathering data

### Custodian-specific versus blanket provisions

US privacy law treats health data differently depending on how they are created and who is handling the data—that is, who is the custodian. By contrast, the EU General Data Protection Regulation sets out a single broadly defined regime for health data (as well as other data), no matter what format, how it is collected, or who the custodian is. (*reference*: Terry, N. P. Existential challenges for healthcare data protection in the United States. Ethics, Med., & Pub. Health 3, 19–27 (2017).)

It defines the category of ‘data concerning health’ broadly to mean “personal data related to the physical or mental health of a natural person, including the provision of health care services, which reveal information about his or her health status.” (*reference*: Commission Regulation 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with regard to the Processing of Personal Data and on the Free Movement of such Data, and Repealing Directive, 95/46/EC, 2016 O.J. (L 119) 1, 34 (General Data Protection Regulation). http://www.privacy-regulation.eu/en/article-4-definitions-GDPR.htm (2016).)

HIPAA의 맹점
1) deidentified data may become reidentifiable through data triangulation from other datasets.
2) focuses its regulation on particular actors and their activities, not the data themselves.
3) more fundamental problem: the majority of health data is not covered by HIPPA at all.

### Equitable data collection

Marginalized populations that are missing from non-health data such as credit card use or Internet history—leading to biases in credit scores or consumer profiles—may also be absent from big health data, such as genomic databases or EHRs, due in part to lack of health insurance and the inability to access health care as well as a number of other reasons.

### The role of the patient in data collection and access

One might think that individually unconsented use is more appropriate (especially for relatively deidentified data) the more the contributing patient will benefit from the data use—a principle of reciprocity—and where the risks to the patient are low, such that the ‘ask’ of patients is small compared to the benefit—a principle of proportionality.

Second, whether or not patients consent for their data to be included within a set, what role should they have in deciding what kind of uses of their data are permissible?  

While approaches built on any of these models may be feasible at the current moment, they may be less feasible in a future where datasets—containing not only huge amounts but huge varieties of data—are used for multiple different analyses.

## Data uses

### Discrimination based upon health data

GINA, ADA, PPACA - represent an attemp to limit consequentialist privacy harms by limiting consequences of access to data rather than focusing on protecting data themselves.  
하지만, 위의 법령도 명백한 허점이 있다.  

### Sharing of private information

Big data also raises the possibility of more dignitary harms.
This example also helpfully illustrates the problem that information shared about one individual may reveal information about other individuals—here, genetic relatives—who are unaware that potentially revealing information has been shared and who have not consented to the sharing.

A more subtle and more difficult issue raised by predictive analytics is whether a person's privay is breached when others make inferences about this individual.

## A path forward

Under this approach, perhaps data sharing should be limited to the minimal amount necessary in all contexts, data should be retained only for limited time, or data should be intentionally obfuscated if consequential harms are difficult to limit. Nevertheless, we argue that limits on data access can bring their own harms.  
Patchy, fragmented health data make data-driven innovation hard, imposing both technological and economic hurdles.  
In some contexts, researchers could use techniques involving pseudonymized data or differential privacy rather than identified data.  
When big data yields surprising insights about how to provide care, providers and patients need to trust the results to implement them. This already creates challenges when the insights come from explicit analyses of big data; when machine-learning and opaque algorithms are involved, trust may be even harder to engender.  
 
