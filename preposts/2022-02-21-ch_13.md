---
layout  : wiki
title   : "An introduction to GLM Chapter 13."
summary : "Chapter 13. Markov chain Monte Carlo methods"
date    : 2022-02-21 06:10:33 +0900
updated : 2022-02-21 07:31:36 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-introduction_GLM]]
latex   : false
---

## 13.1 Why standard inference fails

calculating complex integrals and hence making inference about $\boldsymbol\theta$.

* Equation 13.1
$$P(\theta|\mathbf{y})=\frac{P(\mathbf{y}|\theta)P(\theta)}{\int P(\mathbf{y}|\theta)P(\theta)d\theta}$$

## 13.2 Monte Carlo integration

If a histogram is a reasonable approximation to a continuous distribution, then we can make any inferences about $P(\theta)$.  

* Equation 13.2
$$\hat{\bar\theta}=\frac{1}{M}\sum_{i=1}^M\theta^{(i)}$$

## 13.3 Markov chains

drawing samples directly from the **target density** $P(\theta)$.  
future state is only dependent on the current state and is independent of the past  

### 13.3.1 The Metropolis-Hastings sampler

randomly proposing a new value $\theta^*$.

### 13.3.2 The Gibbs sampler

It splits the parameters into a number of components and then updates each one in turn.  

