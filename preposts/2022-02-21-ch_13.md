---
layout  : wiki
title   : "An introduction to GLM Chapter 13."
summary : "Chapter 13. Markov chain Monte Carlo methods"
date    : 2022-02-21 06:10:33 +0900
updated : 2022-02-21 13:15:50 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-introduction_GLM]]
latex   : false
---

## 13.1 Why standard inference fails

calculating complex integrals and hence making inference about $\boldsymbol\theta$.

* Equation 13.1
$$P(\theta|\mathbf{y})=\frac{P(\mathbf{y}|\theta)P(\theta)}{\int P(\mathbf{y}|\theta)P(\theta)d\theta}$$

## 13.2 Monte Carlo integration

If a histogram is a reasonable approximation to a continuous distribution, then we can make any inferences about $P(\theta)$.  

* Equation 13.2
$$\hat{\bar\theta}=\frac{1}{M}\sum_{i=1}^M\theta^{(i)}$$

## 13.3 Markov chains

drawing samples directly from the **target density** $P(\theta)$.  
future state is only dependent on the current state and is independent of the past  

### 13.3.1 The Metropolis-Hastings sampler

randomly proposing a new value $\theta^*$.

### 13.3.2 The Gibbs sampler

It splits the parameters into a number of components and then updates each one in turn.  

### 13.3.3 Comparing a Markov chain to classical maximum likelihood estimation

iterative Newton-Raphson algorithm was only concerned with one aspect of the joint distribution.  
The cost of the extra information is the computation.  

Newton-Raphson: deterministic steps, assuming `fixed likelihood` and initial values.  
Markov chain: takes `random` steps

### 13.3.4 Importance of parameterization

The efficiency of the Markov chain sampling is dependent on (amongst other things) the model parameterization.

## 13.4 Bayesian inference

This ease of making complex inference is one of the major advantages of using MCMC methods.  
We can estimate a likely range for $\boldsymbol\beta$ using a `posterior interval`

## 13.5 Diagnostics of chain convergence

*Assumption)* the sample densities for the unknown parameters were good estimates of the target densities.  
when a chain has **converged** to the target density.

### 13.5.1 Chain history

A simple (informal) method of assessing chain convergence is to look at the `history` of iteration using a time series plot.  
By `centring` the dose covariate we have greatly improved the convergence because centring reduces the correlation between the parameter estimates.  

### 13.5.2 Chain autocorrelation

Autocorrelation is a useful diagnostics because it summarizes the dependence between neighbouring samples.  
`thinning`: reduce it by systematically usign every $j$th sample and discarding the others.  

### 13.5.3 Multiple chains

particularly good for assessing the influence of initial values.  
*difficult* to generate suitably varied starting values, with many unknown parameters and multi-dimensional likelihoods.

## 13.6 Bayesian model fit: the deviance information criterion

`DIC`: generalization of the AIC  

* Equation 13.4
$$DIC = D(\mathbf{y}|\bar\boldsymbol\theta) + 2p_{D}$$

effective number of parameters is estimated:  

* Equation 13.5
$$p_{D}=\bar{D(\mathbf{y}|\boldsymbol\theta)} - D(\mathbf{y}|\bar\boldsymbol\theta)$$

* Equation 13.6
$$DIC=\bar{D({y}|\theta)}+p_{D}$$

The DIC is a guide to model selection and should not be treated as an absolute decision criterion.

