---
layout  : wiki
title   : "An introduction to GLM Chapter 9"
summary : "Chapter 9 Poisson regression and log-linear models"
date    : 2022-01-31 10:38:22 +0900
updated : 2022-02-02 10:56:53 +0900
tag     : 
toc     : true
public  : true
parent  : [[2022-01-10-introduction_GLM]] 
latex   : false
---

# Chapter 9. Poisson regression and log-linear models

## 9.1 Introduction

The number of times an event occurs is a common form of data.  
The Poisson distribution is often used to model count data. If $Y$ is the number of occurrences, its probability distribution:  
$$f(y)=\frac{\mu^y e^{-\mu}}{y!},\,y=0,1,2,...,$$
where $\mu$ is the average number of occurrences.  
Often it needs to be described as a **rate**.  
The **time scale** should be included in the definition.  
More generally, the rate is specified in terms of units of "exposure".  
So the rate may be defined in terms of pearson-year "at risk".  

The events relate to varying amounts of exposure which need to be taken into account when modelling the rate of events. - Poisson regression  
Exposure is constant (and not relevant to the model) and the explanatory variables are usually categorical. The response variable is the frequency or count in each cell of the table. - log-linear model  

## 9.2 Poisson regression

The dependence of $\theta_i$ on the explanatory variables,  
* Equation 9.1
$$\theta_i=e^{\mathbf{x}_i^T\beta}$$

The GLM is  
* Equation 9.2
$$E(Y_i)=\mu_i=n_ie^{\mathbf{x}_i^T\beta};\,Y_i\sim Po(\mu_i)$$

The natural link function is the logarithmic function  
* Equation 9.3
$$\log\mu_i=\log{n_i}+\mathbf{x}_i^T\beta$$
$\log{n_i}$ is called **offset**, known constant, which is readily incorporated into the estimation procedure.  

Hypothesis can be tested using the Wald, score or likelihood ratio statistics.  
* Equation 9.4
$$\frac{b_j-\beta_j}{s.e.(b_j)}\sim N(0, 1)$$
Alternatively, hypothesis testing can be performed by comparing the goodness of fit.  
As var$(Y_i)$=E$(Y_i)$ for the Poisson distribution, the standard error of $Y_i$ is estimated $\sqrt{e_i}$ so the Pearson residuals are  
* Equation 9.5
$$r_i=\frac{o_i-e_i}{\sqrt{e_i}}$$
The deviance for a Poisson model is given in Section 5.6.3.  
* Equation 9.6
$$D=2\sum\left[o_i\log(o_i/e_i)-(o_i-e_i)\right]$$
deviance simplifies to  
* Equation 9.7
$$D=2\sum\left[o_i\log(o_i/e_i)\right]$$
**Deviance residuals** are the component of $D$ in (9.6)  
* Equation 9.8
$$d_i=sign(o_i-e_i)\sqrt{2[o_i\log(o_i/e_i)-(o_i-e_i)]},\, i=1,...,N$$

The goodness of fit statistics $X^2$ and $D$ are closely related. Both can be used directly as measures of goodness of fit, as they can be calculated from the data and the fitted model.  
They can be compared with the central chi-squared distribution with $N-p$ degrees of freedom.  
The chi-squared distribution is likely to be a better approximation for the sampling distribution of $X^2$ (NOTE: Section 7.5).  

Two other summary statistics are the likelihood ratio chi-squared statistic and pseudo $R^2$.  
The likelihood ratio chi-squared statistic $C = 2[l(\mathbf{b})-l(\mathbf{b}_{min})]$ provides an overall test of the hypothesis.  
Less formally, pseudo $R^2 = [l(\mathbf{b}_{min})-l(\mathbf{b})]/l(\mathbf{b}_{min})$ provides an intuitive measure of fit.

### 9.2.1 Example of Poisson regression: British doctors' smoking and coronary death

Building a model  
Calculating Wald statistics  
Calculating C statistics and pseudo $R^2$  

## 9.3 Examples of contingency tables

How the design of the study may determine constraints on the data. The study design also affects the choice of probability models to describe the data.

### 9.3.1 Example: Cross-sectional study of malignant melanoma

The joint probability distribution of the $Y_{jk}$'s, conditional on their sum $n$, is the Multinomial distribution  
$$f(\mathbf{y}|n)=n!\prod_{j=1}^J\prod_{k=1}^K\theta_{jk}^{y_{jk}}/y_{jk}!$$
where $\theta_{j k}=\mu_{j k}/\mu$

### 9.3.2 Example: Randomized controlled trial of influenza vaccine

The row totals are fixed. **Product multinomial distribution**  

$$\log{E(Y_{jk})}=\log\mu_{jk}=\log{y_j}+\log{\theta_{jk}}$$

### 9.3.3 Example: Case control study of gastric and duodenal ulcers and aspirin use

This is another form of **product multinomial distribution**

$$\log{E(Y_{jkl})}=\log{y_{jk}}+\log{\theta_{jkl}}$$

## 9.4 Probability models for contingency tables

### 9.4.1 Poisson model

If there were no constraints on the $Y_i$, they could be modelled as independent random variables with the parameters $E(Y_i)=\mu_i$  

### 9.4.2 Multinomial model

If the only constraint is that the sum of the $Y_i$'s is n  
row and column variables are independent  

### 9.4.3 Product multinomial models

for a three dimensional table with $J$ rows, $K$ columns and $L$ layers, if the row totals are fixed in each layer,  

## 9.5 Log-linear models

All the probability models are based on the Poisson distribution and in all cases $E(Y_i)$ can be written as a producto of parameters and other terms. Thus, the natural link function for the Poisson distribution, the logarithmic function, yields a linear component  
$$\log{E(Y_i)}=constant\, +\, \mathbf{x}_i^T\beta$$
The term **log-linear model** is used to describe all these generalized linear models.  

The hypothesis of independence can be tested by comparing the additive model (on the logarithmic scale)  
* Equation 9.10
$$\log{E(Y_{jk})}=\log{n}+\log{\theta_{j.}}+\log{\theta_{.k}}$$
with the model  
* Equation 9.11
$$\log{E(Y_{jk})}=\log{n}+\log{\theta_{jk}}$$

This is analogous to analysis of variance for a two factor experiment without replication (NOTE: Section 6.4.2).  



